import os
import csv
import re
import pandas as pd
from pathlib import Path
import geopandas as gpd
from shapely.geometry import Point
import subprocess
import json
import re
import glob
import yaml
import sys
import time
import copy
from google.cloud import storage

# Load configuration file
def load_config(config_path):
    with open(config_path, "r") as file:
        config = yaml.safe_load(file)
    return config

def get_utm_zone_from_gdf(gdf):
    if gdf.crs == 'EPSG:4326':
        utm_zone = f'EPSG:326{int(gdf.geometry.x.mean() // 6 + 31)}'
    else:
        return None
    return utm_zone

def list_folders_two_levels_deep(path):
    """
    List all folders that are two levels deep and start with 8 digits (in the format yyyymmdd).

    :param path: Path to the directory to list folders in.
    :return: List of folder paths that match the date format.
    """

    print(f"Listing folders in path: {path}")

        # --- Check if the path is for Google Cloud Storage ---
    if path.startswith('gs://'):
        print(f"Listing all blobs in GCS path: {path}")
        
        # A set to store the unique folder paths we find to avoid duplicates
        matching_folders = set()
        
        try:
            # Use the explicit project and user_project for robust authentication
            storage_client = storage.Client()
            bucket_name, prefix = path.replace('gs://', '').split('/', 1)
            if not prefix.endswith('/'):
                prefix += '/'
            bucket = storage_client.bucket(bucket_name)

            # List ALL blobs recursively (no delimiter)
            all_blobs = bucket.list_blobs(prefix=prefix)

            # This regex will find folders that start with 8 digits
            dir_pattern = re.compile(r'(\d{4})')

            for blob in all_blobs:
                blob_parts = blob.name.split('/')
                if len(blob_parts) > 2:
                    first_level_folder = blob_parts[2]
                    second_level_folder = blob_parts[3]                    
                    # Check if the first folder part matches the date pattern
                    if dir_pattern.match(first_level_folder):
                        
                        # --- This is the correct way to construct the final path ---
                        final_path = f"gs://{bucket_name}/{prefix}{first_level_folder}/{second_level_folder}/01_Images/{second_level_folder}/"
                        matching_folders.add(final_path)

            print(f"Found {len(matching_folders)} unique matching folders.")
            return list(matching_folders)

        except Exception as e:
            print(f"An error occurred while listing GCS folders: {e}")
            return []
        # --- If not a GCS path, assume it's a local/HPC filesystem path ---
    else:
        print("Local filesystem path detected. Using 'os' module.")
        # --- SLURM/LOCAL LOGIC ---
        date_regex = re.compile(r'^\d{4}')
        matching_folders = []
        try:
            for first_level_folder in os.listdir(path):
                first_level_folder_path = os.path.join(path, first_level_folder)
                if os.path.isdir(first_level_folder_path) and date_regex.match(first_level_folder):
                    for second_level_folder in os.listdir(first_level_folder_path):
                        second_level_folder_path = os.path.join(first_level_folder_path, second_level_folder)
                        if os.path.isdir(second_level_folder_path):
                            # Construct the final path for the Slurm environment
                            final_path = os.path.join(second_level_folder_path, '01_Images', second_level_folder, '')
                            matching_folders.append(final_path)
            return matching_folders

        except FileNotFoundError:
            print(f"Error: The path '{path}' does not exist.")
            return []
    

def list_orthomosaics(om_path, maptiles_path):
    om_dict = {}

    for filename in os.listdir(om_path):
        print(f'filename: {filename}')
        if filename.endswith("aligned.tif"):
            parts = filename.split('_')
            print(f'parts: {parts}')
            if len(parts) >= 4:  # Ensure enough parts for farm_field, sensor, date
                try:
                    date = parts[-2].split('.')[0] # Get date and remove .tif
                    sensor = parts[-2]
                    farm_field_parts = parts[:-2]
                    farm_field = "_".join(farm_field_parts)

                    # Check if maptiles have already been generated by checking for the existence of folder in maptiles_path
                    # The folder will be maptiles_path/farm_field/date
                    
                    # Check if maptiles exist for this orthomosaic
                    maptile_farmfield = os.path.join(maptiles_path, farm_field)
                    maptile_farmfield_exists = os.path.exists(maptile_farmfield)
                    maptile_path = os.path.join(maptiles_path, farm_field, date)
                    maptiles_exist = os.path.exists(maptile_path)
                    
                    # Skip if date for maptiles already exist or orthomosaic is not rgb or  farmfield folder does not exist 
                    if maptiles_exist or not sensor == 'rgb' or not maptile_farmfield_exists:
                        continue


                    if farm_field not in om_dict:
                        om_dict[farm_field] = {}

                    # Store the full filename or just the date part as value if preferred
                    om_dict[farm_field][date] = filename 
                    print(f'adding {filename} to dictionary')
                except IndexError:
                    print(f"Warning: Could not parse filename: {filename}. Skipping.")
                except ValueError:
                    print(f"Warning: Could not parse date from filename: {filename}. Skipping.")
            else:
                print(f"Warning: Filename {filename} does not have enough parts. Skipping.")
    
    return om_dict

    
def geotagged_folders(folders, file_age = 3600):
    valid_folders = []
    invalid_folders = []

    # Determine the platform once based on the first path in the list
    is_gcs = folders[0].startswith('gs://') if folders else False
    
    if is_gcs:
        # Initialize GCS client once if needed
        storage_client = storage.Client(project='uas-orchestration-engine')
        bucket_name = folders[0].replace('gs://', '').split('/')[0]
        print(f'GCS bucket name: {bucket_name}')

    for folder in folders:
        if is_gcs:
            # --- GCP Logic ---
            output_prefix = f"{folder.rstrip('/')}/OUTPUT/"
            print(f' output_prefix: {output_prefix}')
            blobs = list(storage_client.list_blobs(bucket_name))

            #print(f'blobs in {output_prefix}: {blobs}')
            
            if not blobs:
                print(f'no output folder exists for {folder}')
                invalid_folders.append(folder)
                continue

            pdf_exists = any(b.name.lower().endswith('.pdf') for b in blobs)
            csv_blobs = [b for b in blobs if b.name.lower().endswith('.csv')]
            last_modified_time = max((b.updated.timestamp() for b in blobs), default=0)

        else:
            # --- Slurm/Local Logic ---
            output_path = os.path.join(folder, 'OUTPUT')
            
            if not os.path.isdir(output_path):
                print(f'no output folder exists for {folder}')
                invalid_folders.append(folder)
                continue
            
            files = os.listdir(output_path)
            pdf_exists = any(f.lower().endswith('.pdf') for f in files)
            csv_files = [f for f in files if f.lower().endswith('.csv')]
            last_modified_time = max((os.path.getmtime(os.path.join(output_path, f)) for f in files), default=0)

        # --- Common Validation Logic ---
        if time.time() - last_modified_time < file_age:
            print(f"Skipping {folder}: Files are too new.")
            continue
        
        if pdf_exists and (csv_files if not is_gcs else csv_blobs):
            valid_folders.append(folder)
        else:
            invalid_folders.append(folder)
            
    return valid_folders, invalid_folders

def make_dict(valid_folders, flight_config_dict, sensor_dict, om_folder, regen_shell_scripts = False, date_range = None, location_filter = None):
    count = 0
    flight_dict = {}
    folders_not_matched = []
    folders_multiple = []
    om_remaining = []
    folders_remaining = []

    # Initialize the nested keys in the flight_config_dict in the form of[flight][orthomosaic][date]
    for flight, value in flight_config_dict.items():
        if location_filter and flight != location_filter:
            print(f'skipping {flight} because it is not {location_filter}')
            continue
        if flight not in flight_dict:
            flight_dict[flight]= {}

            for om in value.keys():
                if om not in flight_dict[flight]:
                    flight_dict[flight][om] = {}

    for folder in sorted(valid_folders):
        folder_match = False
        folder_parts = folder.strip('/').split('/')

        for flight, value in flight_config_dict.items():
            if location_filter and flight != location_filter:
                print(f'skipping {flight} because it is not {location_filter}')
                continue
            for om in value.keys():
                # Find the farm and field
                farm_field = flight.split('_')
                farm = farm_field[0]
                if len(farm_field) == 2:
                    field = farm_field[1]
                    if field == 'n':
                        field = '_n_'
                else:
                    field  = None
                    #Find the field
                if farm in folder_parts[8].lower():
                    if field is None or field in folder_parts[9].lower(): 
                        date = folder_parts[8][:8]

                        for sensor_type, sensor_name in sensor_dict.items():
                            if sensor_name in folder_parts[9].lower():
                                if sensor_type not in flight_dict[flight][om]:
                                    flight_dict[flight][om][sensor_type] = {}
                                if date not in flight_dict[flight][om][sensor_type]:
                                    flight_dict[flight][om][sensor_type][date] = {}
                                    folder_match = True
                                    tif_file = os.path.join(om_folder, f'{om}_{sensor_type}_{date}.tif')
                                    if (date_range and date >= date_range[0] and date <= date_range[1]) or not date_range:  # If date_range is not provided, process all dates
                                        if not os.path.exists(tif_file) or regen_shell_scripts:
                                            #print(f'DOES NOT EXIST: {om}_{sensor_type}_{date}.tif')
                                            om_remaining.append(f'{om}_{sensor_type}_{date}.tif')
                                            folders_remaining.append(folder)
                                            count += 1
                                            flight_dict[flight][om][sensor_type][date]['status'] = 'validated'
                                            flight_dict[flight][om][sensor_type][date]['input_path'] = folder
                                        #else:
                                        #    print(f'Orthomosaic for {location}_{sensor_type}_{date} already exists')
                                        # Update dictionary to include each step for a given date to note if it is complete or not. 
                                        # for step in uas_pipeline:
                                        # Check if each output_folder/file exists and mark the step as compleete or incomplete.
                                    # If the date already exists in the dictionary, mark this one as a multiple
                                else:
                                    print(f'folder multiple: {folder}')
                                    folders_multiple.append(folder)
                                    consolidate_images(folder, flight, sensor_type, sensor_name, date)
                                    folder_match = True

        if folder_match == False:
            folders_not_matched.append(folder)                

    print(f'number of geotagged folders remaining to generate orthomosaic: {count}')
    return flight_dict, count, folders_not_matched, folders_multiple, om_remaining, folders_remaining

def om_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):
    # Check if the orthomosaic exists in the om_folder
    # If it does, check if the orthomosaic is complete
    # If it is not complete, add it to the om_remaining list
    # If it is complete, add it to the flight_dict
    # If it does not exist, add it to the om_remaining list
    # Return the flight_dict, om_remaining, and folders_remaining

    # Get the om_folder from the config
    output_dir = os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder'])
    print(f'output dir from om_validation: {output_dir}')
    output_file_template = config['uas_pipeline'][step]['output_file']

    # Get the flight_config_dict from the config
    flight_config_dict = config['flight_list']
    # Get the sensor_dict from the config
    sensor_dict = config['sensor_dict']
    # Get the date_range from the config

    folders_not_matched = []
    folders_multiple = []
    om_remaining = []
    folders_remaining = []
    count = 0

    flights_folder = os.path.join(config['base_folder'], 'flights')
    folders = list_folders_two_levels_deep(flights_folder)

    valid_folders, invalid_folders = geotagged_folders(folders, file_age = file_age)

    print(f'number of valid folders: {len(valid_folders)}')
    print(f'number of invalid folders: {len(invalid_folders)}')

    for folder in sorted(invalid_folders):
        folder_only = folder.split('/')[-2]
        print(f'invalid folder: {folder_only}')

    # Iterate through the valid folders and add to the flight_dict
    for folder in sorted(valid_folders):
        folder_match = False
        folder_parts = folder.strip('/').split('/')

        print(f'folder parts: {folder_parts}')
        print(f'folders parts -3: {folder_parts[-3]}')
        print(f' folder parts -4: {folder_parts[-4]}')

        # Iterate through the flight_config_dict and add to the flight_dict
        for flight, value in flight_config_dict.items():
            for om in value.keys():
                # Find the farm and field
                farm_field = flight.split('_')
                farm = farm_field[0]
                if len(farm_field) == 2:
                    field = farm_field[1]
                    if field == 'n':
                        field = '_n_'
                else:
                    field  = None
                    #Find the field
                if farm in folder_parts[-4].lower():
                    if field is None or field in folder_parts[-3].lower(): 
                        date = folder_parts[-3][:8]

                        for sensor_type, sensor_name in sensor_dict.items():
                            if sensor_name in folder_parts[-3].lower():
                                if sensor_type not in flight_dict[flight][om]:
                                    flight_dict[flight][om][sensor_type] = {}
                                if date not in flight_dict[flight][om][sensor_type]:
                                    flight_dict[flight][om][sensor_type][date] = {}
                                    folder_match = True
                                    output_filename = output_file_template.format(om=om, sensor_type=sensor_type, date=date)
                                    output_file = os.path.join(output_dir, output_filename)
                                    #print(f'output_file: {output_file}')
                                    if not os.path.exists(output_file):
                                        if step not in flight_dict[flight][om][sensor_type][date]:
                                            flight_dict[flight][om][sensor_type][date][step] = {}
                                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                                        #print(f'DOES NOT EXIST: {output_filename}')
                                        om_remaining.append(output_filename)
                                        folders_remaining.append(folder)
                                        count += 1
                                    else:
                                        if step not in flight_dict[flight][om][sensor_type][date]:
                                            flight_dict[flight][om][sensor_type][date][step] = {}
                                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'

                                    # Add the input path to the flight_dict
                                    if flight_config_dict[flight][om]:
                                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = f'{folder}OUTPUT_{om}/'
                                    else:
                                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = f'{folder}OUTPUT/'
                                else:
                                    print(f'This folder is a multiple, needs to be consolidated: {folder}')
                                    print(f'flight: {flight}, om: {om}, sensor_type: {sensor_type}, date: {date}')
                                    folders_multiple.append(folder)
                                    if confirm_action:
                                        confirm = input("Do you want to consolidate this folder? (y/n): ").lower()
                                        # Check if the user confirmed
                                        if confirm == 'y':
                                            print("--> Consolidating images...")
                                            folders_multiple.append(folder)
                                            consolidate_images(folder, flight, sensor_type, sensor_name, date)
                                            folder_match = True
                                        else:
                                            print("--> Skipping consolidation.")
                                    else:
                                        folders_multiple.append(folder)
                                        consolidate_images(folder, flight, sensor_type, sensor_name, date)
                                        folder_match = True

        if folder_match == False:
            folders_not_matched.append(folder)
            print(f'folder not matched: {folder}')              

    return flight_dict

def om_align_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

    #Get the previous step from the config
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    #Get the output folders from the config
    input_dir = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])
    output_dir = Path(os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder']))
    shapefiles_alignment_folder = os.path.join(config['base_folder'], config['uas_pipeline'][step]['shapefiles_alignment_folder'])
    shapefiles_alignment_format = config['uas_pipeline'][step]['shapefiles_alignment_format']
    

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                # To keep track of the input file from the last iteration
                previous_input_file = None
                # Sort the dates to ensure chronological order
                sorted_dates = sorted(value3.keys())

                shapefile_alignment = os.path.join(shapefiles_alignment_folder, shapefiles_alignment_format.format(om=om))

                for date in sorted_dates:
                    flight_dict[flight][om][sensor_type][date][step] = {}
                    if previous_input_file:
                        alignment_file = output_dir / Path(previous_input_file).name.replace('.tif', '_aligned.tif')
                    else:
                        alignment_file = None

                    input_file_template = config['uas_pipeline'][step_dependency]['output_file']
                    input_filename = input_file_template.format(om=om, sensor_type=sensor_type, date=date)  
                    input_file = os.path.join(input_dir, input_filename)

                    output_file_template = config['uas_pipeline'][step]['output_file']
                    output_filename = output_file_template.format(om=om, sensor_type=sensor_type, date=date)
                    output_file = os.path.join(output_dir, output_filename)

                    # Store the current input file to be used as the alignment file in the next iteration
                    previous_input_file = input_file

                    flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_file
                    flight_dict[flight][om][sensor_type][date][step]['alignment_path'] = alignment_file

                    # First, determine the status of the necessary inputs
                    # This handles the case where alignment_file might be None
                    inputs_ready = False
                    if alignment_file is None:
                        # First date, only input_file is required
                        if os.path.exists(input_file):
                            inputs_ready = True
                    else:
                        # Subsequent dates, both files are required
                        if os.path.exists(input_file) and os.path.exists(alignment_file):
                            inputs_ready = True

                    # Now, set the status based on the inputs and the output
                    output_exists = os.path.exists(output_file)

                    if not os.path.exists(shapefile_alignment):
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'missing_shapefile_alignment'
                    elif inputs_ready and output_exists:
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                    elif inputs_ready and not output_exists:
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                    elif not inputs_ready and not output_exists:
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                    else: # This implicitly covers 'not inputs_ready and output_exists'
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def plottiles_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):
    
    #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    # Get the sensor(s) this step applies to from the config
    applicable_sensors = config['uas_pipeline'][step].get('sensor')
    if not applicable_sensors:
        # This is a safeguard in case the 'sensor' key is missing in the config for this step
        print(f"Warning: No 'sensor' key configured for {step}. Skipping this validation.")
        return flight_dict

    #Get the output folders from the config

    input_file_template = config['uas_pipeline'][step_dependency]['output_file']
    input_dir_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])

    plots_dict = config['plot_shapefiles']

    source = config['uas_pipeline'][step]['source']


    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            shapefiles_exist = False
            if om in plots_dict:
                shapefiles_exist = True
                plots_shapefiles = plots_dict[om]

                for crop, shapefile in plots_shapefiles.items():
                    shapefile = os.path.join(config['base_folder'], shapefile)
                    #print(f'plot shapefile: {shapefile}')
                    if not os.path.exists(shapefile):
                        shapefiles_exist = False
                        break
                    
            #print(f'shapefiles exist: {shapefiles_exist}')

            for sensor_type, value3 in value2.items():
                if applicable_sensors and sensor_type not in applicable_sensors:
                    continue
                for date, _ in value3.items():
                    flight_dict[flight][om][sensor_type][date][step] = {}
                    output_dir_template = config['uas_pipeline'][step]['output_folder']
                    output_dirname = output_dir_template.format(om=om, sensor_type=sensor_type, date=date, source = source)
                    output_dir = os.path.join(config['base_folder'], output_dirname)
                    input_filename = input_file_template.format(om=om, sensor_type=sensor_type, date=date)
                    input_dir = input_dir_template.format(om=om, sensor_type=sensor_type, date=date)
                    input_file = os.path.join(input_dir, input_filename)

                    if step == 'step3':
                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_file
                        input_path = input_file
                    elif step == 'step6':
                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_dir
                        input_path = input_dir
                    else:
                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = ""


                    # Check if output dir does not exist and input file exists
                    if not shapefiles_exist:
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'missing_plot_shapefiles'
                    elif not os.path.exists(output_dir) and os.path.exists(input_path):
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                    # Check if output dir does not exist and input file does not exist
                    elif shapefiles_exist and not os.path.exists(output_dir) and not os.path.exists(input_path):
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                    # Check if output dir exists and input file exists if it does check that output plot tiles matches input plot polygons
                    elif os.path.exists(output_dir) and os.path.exists(input_path):
                        file_count = len(os.listdir(output_dir))

                        # Count the shapes in the input shapefiles
                        shapefiles = []
                        for _, shapefile in config['plot_shapefiles'][om].items():
                            shapefiles.append(os.path.join(config['base_folder'], shapefile))
                        shape_count = 0
                        for shapefile in shapefiles:
                            shape_count += len(gpd.read_file(shapefile))

                        # If the file_count is equal to the shape_count, then the step is complete
                        if file_count == shape_count:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                        # Otherwise, flag an error
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = f'error - filecount {file_count} != shape_count {shape_count}'
                    else:
                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

        
    return flight_dict

def dgr_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):
    
    step_sensor = config['uas_pipeline'][step]['sensor'][0]
    image_extensions = ('.tif', '.jpg', '.jpeg')

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}
                        output_dir_template = config['uas_pipeline'][step]['output_folder']
                        output_dirname = output_dir_template.format(om=om, sensor_type=sensor_type, date=date)
                        output_dir = os.path.join(config['base_folder'], output_dirname)
                        input_folder = flight_dict[flight][om][sensor_type][date]['step1']['input_path']
                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_folder

                        img_dir = input_folder
                        """if os.path.exists(f'{input_folder}OUTPUT_{om}'):
                            img_dir = f'{input_folder}OUTPUT_{om}/'
                        else:
                            img_dir = f'{input_folder}OUTPUT/'"""

                        # Check if output dir does not exist and input folder exists
                        if not os.path.exists(output_dir) and os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_dir) and not os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        # Check if output dir exists and input file exists if it does check that output images matches input images
                        elif os.path.exists(output_dir) and os.path.exists(input_folder):
                            # image count in output directory
                            output_image_count = sum(1 for image in os.listdir(output_dir) if image.lower().endswith(image_extensions))

                            # image count in input folder 
                            input_image_count = sum(1 for image in os.listdir(img_dir) if image.lower().endswith(image_extensions))

                            # Count rows in csv files
                            csv_files = [file for file in os.listdir(img_dir) if file.lower().endswith('.csv')]
                            total_rows = 0
                            for csv_file in csv_files:
                                df = pd.read_csv(os.path.join(img_dir, csv_file))
                                total_rows += len(df)

                            # If the output image count is equal to the input image count, then the step is complete
                            if output_image_count == 0:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'error: no images'
                            elif output_image_count == total_rows:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                            
                            # Otherwise, flag an error
                            else:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = f'error: {total_rows} csv_rows != {output_image_count} images'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def ir_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

    #Get the step dependency
    omalign_dependency = config['uas_pipeline'][step]['step_dependency'][0]
    dgr_dependency = config['uas_pipeline'][step]['step_dependency'][1]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]


        #Get the output folders from the config
    omalign_folder = os.path.join(config['base_folder'], config['uas_pipeline'][omalign_dependency]['output_folder'])
    dgr_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][dgr_dependency]['output_folder'])
    omalign_file_template = config['uas_pipeline'][omalign_dependency]['output_file']

    output_folder_template = config['uas_pipeline'][step]['output_folder']

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        dgr_foldername = dgr_folder_template.format(om=om, sensor_type=sensor_type, date=date)
                        dgr_folder = os.path.join(config['base_folder'], dgr_foldername)
                        omalign_filename = omalign_file_template.format(om=om, sensor_type=sensor_type, date=date)
                        omalign_file = os.path.join(omalign_folder, omalign_filename)
                        output_foldername = output_folder_template.format(om=om, sensor_type=sensor_type, date=date)
                        output_folder = os.path.join(config['base_folder'], output_foldername)

                        flight_dict[flight][om][sensor_type][date][step]['alignment_path'] = omalign_file
                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = dgr_folder

                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_folder) and os.path.exists(omalign_file) and os.path.exists(dgr_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_folder) and not os.path.exists(omalign_file) and os.path.exists(dgr_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif not os.path.exists(output_folder) and os.path.exists(omalign_file) and not os.path.exists(dgr_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif not os.path.exists(output_folder) and not os.path.exists(omalign_file) and not os.path.exists(dgr_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_folder) and os.path.exists(omalign_file) and os.path.exists(dgr_folder):
                            # Count input images
                            input_image_count = sum(1 for image in os.listdir(dgr_folder) if image.endswith('.tif') or image.endswith('.jpg') or image.endswith('.JPG') or image.endswith('.jpeg'))
                            
                            # Count output images
                            output_image_count = sum(1 for image in os.listdir(output_folder) if image.endswith('.tif') or image.endswith('.jpg') or image.endswith('.JPG') or image.endswith('.jpeg'))
                            # Count it complete if more than 10% of the input images are in the output folder
                            if input_image_count < 10 * output_image_count:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                            else:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'error - not enough images'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def plot_patches_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

    #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]

    source = config['uas_pipeline'][step]['source']

    #Get the output folders from the config
    input_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])
    output_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder'])

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        output_folder = output_folder_template.format(om=om, sensor_type=sensor_type, date=date, source = source)
                        input_folder = input_folder_template.format(om=om, sensor_type=sensor_type, date=date, source = source)

                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_folder


                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_folder) and os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_folder) and not os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_folder) and os.path.exists(input_folder):

                            # The generator yields '1' for each match, and sum() adds them up.
                            input_image_count = sum(1 for image in os.listdir(input_folder) if image.endswith('.tif') or image.endswith('.jpg') or image.endswith('.JPG') or image.endswith('.jpeg'))
                            output_image_count = sum(1 for image in os.listdir(output_folder) if image.endswith('.tif') or image.endswith('.jpg') or image.endswith('.JPG') or image.endswith('.jpeg'))

                            if input_image_count < .1 * output_image_count:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                            else:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = f'error, {input_image_count} input images < .1 * {output_image_count} output images'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def inf_gs_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

    #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]

    #Get the output folders from the config
    input_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])
    output_folder = os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder'])

    source = config['uas_pipeline'][step]['source']

    output_file_template = config['uas_pipeline'][step]['output_file']

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        output_filename = output_file_template.format(om=om, source=source, date=date)
                        output_file = os.path.join(output_folder, output_filename)
                        input_folder = input_folder_template.format(om=om, sensor_type=sensor_type, date=date, source = source)

                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_folder

                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_file) and os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_file) and not os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_file) and os.path.exists(input_folder):
                            try:
                                with open(output_file, 'r') as f:
                                    data = json.load(f)
                                
                                # This function will correctly explore both "corn" and "soy" branches
                                # and sum the total number of plots.
                                num_entries = count_keys_by_pattern(
                                    data, 
                                    is_match=lambda k: isinstance(k, str) and k.isdigit() and len(k) == 3
                                )

                                # The rest of your logic remains the same
                                if num_entries >= 8:
                                    # Note: For your example file, the total count is 80 (corn) + 84 (soy) = 164
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                                else:
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = f'error < 16 entries ({num_entries})'
                                    
                            except json.JSONDecodeError:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'error - invalid json'
                            except Exception as e:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = f'error - {e}'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def inf_cc_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

        #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]

    #Get the output folders from the config
    input_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])
    output_folder = os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder'])

    source = config['uas_pipeline'][step]['source']

    output_file_template = config['uas_pipeline'][step]['output_file']

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        output_filename = output_file_template.format(om=om, source=source, date=date)
                        output_file = os.path.join(output_folder, output_filename)
                        input_folder = input_folder_template.format(om=om, sensor_type=sensor_type, date=date, source = source)

                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_folder

                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_file) and os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_file) and not os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_file) and os.path.exists(input_folder):
                            try:
                                with open(output_file, 'r') as f:
                                    data = json.load(f)
                                
                                # This function will correctly explore both "corn" and "soy" branches
                                # and sum the total number of plots.
                                num_entries = count_keys_by_pattern(
                                    data, 
                                    is_match=lambda k: isinstance(k, str) and k.isdigit() and len(k) == 3
                                )

                                # The rest of your logic remains the same
                                if num_entries >= 8:
                                    # Note: For your example file, the total count is 80 (corn) + 84 (soy) = 164
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                                else:
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = f'error < 16 entries ({num_entries})'
                                    
                            except json.JSONDecodeError:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'error - invalid json'
                            except Exception as e:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = f'error - {e}'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def count_keys_by_pattern(data, is_match):
    """
    Recursively traverses a nested structure (dicts and lists) and counts
    dictionary keys that satisfy the is_match function.
    """
    count = 0

    # If the data is a dictionary, check its keys and traverse its values
    if isinstance(data, dict):
        for key, value in data.items():
            # Check if the key itself is a match
            if is_match(key):
                count += 1
            # Recursively count within the value
            count += count_keys_by_pattern(value, is_match)
            
    # If the data is a list, traverse its items
    elif isinstance(data, list):
        for item in data:
            count += count_keys_by_pattern(item, is_match)
            
    return count


def inf_sr_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

    #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]

    #Get the output folders from the config
    input_folder_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'])
    output_folder = os.path.join(config['base_folder'], config['uas_pipeline'][step]['output_folder'])

    output_file_template = config['uas_pipeline'][step]['output_file']

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        output_filename = output_file_template.format(om=om, source="om", date=date)
                        output_file = os.path.join(output_folder, output_filename)
                        input_folder = input_folder_template.format(om=om, sensor_type=sensor_type, date=date, source = 'om')

                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_folder

                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_file) and os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_file) and not os.path.exists(input_folder):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_file) and os.path.exists(input_folder):
                            try:
                                with open(output_file, 'r') as f:
                                    data = json.load(f)
                                
                                # This function will correctly explore both "corn" and "soy" branches
                                # and sum the total number of plots.
                                num_entries = count_keys_by_pattern(
                                    data, 
                                    is_match=lambda k: isinstance(k, str) and k.isdigit() and len(k) == 3
                                )

                                # The rest of your logic remains the same
                                if num_entries >= 8:
                                    # Note: For your example file, the total count is 80 (corn) + 84 (soy) = 164
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'
                                else:
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = f'error < 16 entries ({num_entries})'
                                    
                            except json.JSONDecodeError:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = 'error - invalid json'
                            except Exception as e:
                                flight_dict[flight][om][sensor_type][date][step]['status'] = f'error - {e}'
                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

    return flight_dict

def process_geojson_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):
    return flight_dict

def maptiles_validation(step, flight_dict, config, confirm_action = False, file_age = 3600):

            #Get the step dependency
    step_dependency = config['uas_pipeline'][step]['step_dependency'][0]

    step_sensor = config['uas_pipeline'][step]['sensor'][0]

    #Get the output folders from the config
    input_file_template = os.path.join(config['base_folder'], config['uas_pipeline'][step_dependency]['output_folder'], config['uas_pipeline'][step_dependency]['output_file'])

    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                if sensor_type == step_sensor:
                    for date, _ in value3.items():
                        flight_dict[flight][om][sensor_type][date][step] = {}

                        output_folder = os.path.join(config['base_folder'], config['publishing_folder'][om],om,date)
                        input_file = input_file_template.format(om=om, sensor_type=sensor_type, date=date)

                        flight_dict[flight][om][sensor_type][date][step]['input_path'] = input_file

                        # Check if output dir does not exist and input file exists
                        if not os.path.exists(output_folder) and os.path.exists(input_file):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'validated'
                        # Check if output dir does not exist and input file does not exist
                        elif not os.path.exists(output_folder) and not os.path.exists(input_file):
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'not_ready'
                        elif os.path.exists(output_folder) and os.path.exists(input_file):
                            # Confirm zoom levels from 15 to 24 exist

                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'complete'

                            for zoom in range(15, 25):
                                zoom_folder = os.path.join(output_folder, str(zoom))
                                #Count recursively the number of files in the zoom folder with .png suffix
                                if os.path.exists(zoom_folder):
                                    tile_count = 0

                                    # os.walk() recursively visits every directory starting from zoom_folder
                                    for dirpath, dirnames, filenames in os.walk(zoom_folder):
                                        # 'filenames' is a list of files in the current 'dirpath'
                                        for file in filenames:
                                            if file.endswith('.png'):
                                                tile_count += 1
                                    if tile_count == 0:
                                        flight_dict[flight][om][sensor_type][date][step]['status'] = 'empty_zoom_level'
                                elif not os.path.exists(zoom_folder):
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = 'missing_zoom_level'
                                else:
                                    flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'

                        else:
                            flight_dict[flight][om][sensor_type][date][step]['status'] = 'error'
    return flight_dict


def make_dict_bystep(config, uas_pipeline, confirm_action = False, file_age = 3600):
    flight_dict = {}
    folders_not_matched = []
    folders_multiple = []
    om_remaining = []
    folders_remaining = []

    flight_config_dict = config['flight_list']
    sensor_dict = config['sensor_dict']
    om_folder = config['om_folder']

    validation_dict = {'step1': om_validation,
                       'step2': om_align_validation,
                       'step3': plottiles_validation, # OM based plot tiles
                       'step4': dgr_validation,
                       'step5': ir_validation,
                       'step6': plottiles_validation,  # IR based plot tiles
                       'step7': plot_patches_validation,
                       'step8': plot_patches_validation,  
                       'step9': inf_gs_validation,
                       'step10': inf_cc_validation,
                       'step11': inf_sr_validation,
                       'step12': inf_gs_validation,  
                       'step13': inf_cc_validation,  
                       'step14': process_geojson_validation,
                       'step15': maptiles_validation
                       }

    # Initialize the nested keys in the flight_config_dict in the form of[flight][orthomosaic] and filter by location_filter
    for flight, value in flight_config_dict.items():
        if flight not in flight_dict:
            flight_dict[flight]= {}

            for om in value.keys():
                if om not in flight_dict[flight]:
                    flight_dict[flight][om] = {}

    # Update flight_dict with the results from om_validation
    validation_dispatcher = {}

    checkpoints = []
    for i, step in enumerate(uas_pipeline.keys()):
        print(f'step: {step}')
        checkpoints.append(time.time())
        if i > 0:
            print(f'time to complete step {i-1}: {checkpoints[i] - checkpoints[i-1]:.2f} seconds')
        validation_function = validation_dict[step]
        validation_dispatcher[step] = validation_function
        if not validation_function:
            print(f'No validation function found for {step}')
            continue
        flight_dict = validation_function(step, flight_dict, config, confirm_action = confirm_action, file_age = file_age)

    return flight_dict

def filter_flight_dict(flight_dict, flight_filter = None, date_range = None):

    flight_dict_filtered = {}


    for flight, value in flight_dict.items():
        if flight_filter and flight != flight_filter:
            continue
        for om, value2 in value.items():
            for sensor_type, value3 in value2.items():
                for date, value4 in value3.items():
                    if date_range and (date < date_range[0] or date > date_range[1]):
                        continue
                    # This is the correct and most efficient way to do it
                    flight_dict_filtered.setdefault(flight, {}).setdefault(om, {}).setdefault(sensor_type, {})[date] = value4

    return flight_dict_filtered

def validate_flight_dict(flight_dict, config):
    """
    Recursively validates the flight dictionary by checking the entire dependency chain for each step.

    A step marked 'not_ready' will be changed to 'not_validated' only if one of its
    prerequisites, or a prerequisite of a prerequisite, etc., has a definitive
    failed status. It uses a recursive helper function with memoization to
    efficiently determine if a step's dependency chain is valid.

    Args:
        flight_dict (dict): The dictionary containing flight data and step statuses.
        config (dict): The configuration dictionary containing the uas_pipeline rules.

    Returns:
        dict: A new dictionary with the correctly validated and updated step statuses.
    """
    validated_flight_dict = copy.deepcopy(flight_dict)
    pipeline_config = config['uas_pipeline']
    memoization_cache = {}

    def can_proceed(farm_name, plot_name, sensor_type, date_str, step_name_to_check, sorted_dates_map):
        """
        Recursively determines if a step can proceed by checking its entire dependency chain.
        Uses a cache (memoization) to store results and avoid redundant checks.
        """
        # Create a unique key for caching the result of this specific step
        cache_key = (farm_name, plot_name, sensor_type, date_str, step_name_to_check)
        if cache_key in memoization_cache:
            return memoization_cache[cache_key]

        # Get the step's data from the main dictionary
        current_step_data = validated_flight_dict.get(farm_name, {}).get(plot_name, {}).get(sensor_type, {}).get(date_str, {}).get(step_name_to_check)

        if not current_step_data:
            memoization_cache[cache_key] = False
            return False

        status = current_step_data.get('status')
        
        # Base Cases for the recursion
        if status in {'complete', 'validated'}:
            memoization_cache[cache_key] = True
            return True
        
        if status != 'not_ready':
            # Any other status ('missing_dgr', 'not_validated', etc.) is a hard fail
            memoization_cache[cache_key] = False
            return False

        # Recursive Step: The status is 'not_ready', so we must check its dependencies.
        step_config = pipeline_config.get(step_name_to_check, {})
        dependencies = step_config.get('step_dependency', [])
        date_dependency_type = step_config.get('date_dependency', 'none')

        if not dependencies:
            # No dependencies means it's ready to go.
            memoization_cache[cache_key] = True
            return True

        for dep_step_name in dependencies:
            target_date = date_str
            if date_dependency_type == 'previous':
                current_date_index = sorted_dates_map.get(sensor_type, []).index(date_str)
                if current_date_index > 0:
                    target_date = sorted_dates_map.get(sensor_type)[current_date_index - 1]
                else:
                    continue
            
            # Recursively call the function for the dependency. If any dependency fails, this step fails.
            if not can_proceed(farm_name, plot_name, sensor_type, target_date, dep_step_name, sorted_dates_map):
                memoization_cache[cache_key] = False
                return False
        
        # If all dependencies passed the recursive check, this step is valid.
        memoization_cache[cache_key] = True
        return True

    # --- Main part of the function ---
    # Pre-calculate sorted dates for each sensor to pass to the helper function
    all_sorted_dates = {
        farm: {
            plot: {
                sensor: sorted(dates.keys()) for sensor, dates in sensors.items()
            } for plot, sensors in plots.items()
        } for farm, plots in validated_flight_dict.items()
    }

    # Iterate through every step to update its status based on the recursive check
    for farm, plots in validated_flight_dict.items():
        for plot, sensors in plots.items():
            for sensor, dates in sensors.items():
                for date in dates:
                    for step_name, step_data in dates[date].items():
                        try:
                            if step_data['status'] == 'not_ready':
                                sorted_dates_for_sensor = all_sorted_dates[farm][plot]
                                if not can_proceed(farm, plot, sensor, date, step_name, sorted_dates_for_sensor):
                                    step_data['status'] = 'not_validated'
                        except KeyError:
                            print(f'KeyError: {farm}, {plot}, {sensor}, {date}, {step_name}')
                            continue
    return validated_flight_dict

def get_image_metadata(image_path):
    """
    Extracts latitude, longitude, and date from a geotagged image (.jpg or .tif) using ExifTool.

    Args:
        image_path (str): Path to the image file.

    Returns:
        dict: Dictionary containing latitude, longitude, and date.
    """
    exif_cmd = [
        'exiftool', '-GPSLatitude', '-GPSLongitude', '-DateTimeOriginal', '-json', image_path
    ]

    result = subprocess.run(exif_cmd, capture_output=True, text=True)
    
    try:
        metadata = json.loads(result.stdout)[0]
        lat_dms = metadata.get('GPSLatitude')
        lon_dms = metadata.get('GPSLongitude')

        if lat_dms and lon_dms:
            lat = dms_to_decimal(lat_dms)
            lon = dms_to_decimal(lon_dms)

        return {'latitude': lat, 'longitude': lon}
    except (IndexError, json.JSONDecodeError):
        print(f" Unable to read EXIF metadata for {image_path}")
        return None
    
def dms_to_decimal(dms_str):
    """
    Converts a GPS coordinate from Degrees, Minutes, Seconds (DMS) format to Decimal Degrees (DD).

    Args:
        dms_str (str): DMS coordinate as a string (e.g., "83 deg 45' 22.89\" W")

    Returns:
        float: Decimal degree representation of the coordinate.
    """
    dms_regex = r"(\d+) deg (\d+)' ([\d.]+)\" ([NSEW])"
    match = re.match(dms_regex, dms_str)

    if match:
        degrees = float(match.group(1))
        minutes = float(match.group(2))
        seconds = float(match.group(3))
        direction = match.group(4)

        decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)

        if direction in ["S", "W"]:  # South and West are negative
            decimal *= -1

        return decimal
    else:
        raise ValueError(f"Invalid DMS format: {dms_str}")
    

def create_omspecific_output_folders(flight_dict, flight_config_dict, rerun_boundary = False, confirm_action = False):
    for flight, value in flight_dict.items():
        for om, value2 in value.items():
            #print(f'om: {om}')
            if flight_config_dict[flight][om]:
                boundary_file = flight_config_dict[flight][om]
                print(f'boundary file: {boundary_file}')
                for sensor, value3 in value2.items():
                    print(f'sensor: {sensor}')
                    for date, value4 in value3.items():
                        print(f'date: {date}')
                        omimage_folder = value4['step1']['input_path']
                        # Get the parent folder of the folder
                        source_folder = Path(omimage_folder).parents[0] / 'OUTPUT'
                        if not os.path.exists(omimage_folder) or rerun_boundary:
                            filtered_images = filter_images_by_boundary(om, source_folder, boundary_file, force_exiftags = False)
                            #print(f' Length of filtered_images: {len(filtered_images)}')
                            print(f'The following folder will be created and images will be moved to it: {omimage_folder}')
                            if confirm_action:
                                confirm = input(f'Continue? (y/n): ')
                                if confirm == 'y':
                                    move_images_to_folder(filtered_images, source_folder, om)
                            else:
                                move_images_to_folder(filtered_images, source_folder, om)
                            #print(f'folder: {folder}')
                            # Create csv file with the images that were moved
                        csv_files_source = glob.glob(os.path.join(source_folder, "*geotags.csv"))
                        csv_files_target = glob.glob(os.path.join(omimage_folder, "*geotags.csv"))

                        if not csv_files_target and csv_files_source:
                        # 1. Get a list of image filenames that are now in the target directory.
                            try:
                                moved_image_names = {f for f in os.listdir(omimage_folder) if f.lower().endswith(('.tif', '.jpg', '.jpeg'))}
                                if not moved_image_names:
                                    print("No images found in the target directory. Skipping CSV creation.")
                                    # Use 'continue' or 'return' here depending on your loop structure
                                    continue
                            except FileNotFoundError:
                                print(f"Error: Target directory {omimage_folder} does not exist. Cannot create CSV.")
                                continue
                            
                            # 2. Read all source CSVs into a single DataFrame.
                            df_csv_source = pd.concat((pd.read_csv(f) for f in csv_files_source), ignore_index=True)


                            # 3. Filter the DataFrame to keep only rows corresponding to moved images.
                            # The .isin() method is highly efficient for this task.
                            df_new_csv = df_csv_source[df_csv_source['# image name'].isin(moved_image_names)].copy()


                            # 4. Construct the full output path and save the new, filtered DataFrame.
                            output_csv_name = f'{om}_{sensor}_{date}_geotags.csv'
                            output_csv_path = os.path.join(omimage_folder, output_csv_name)
                            
                            df_new_csv.to_csv(output_csv_path, index=False)
                            
                            print(f"Successfully created '{output_csv_name}' with {len(df_new_csv)} entries.")




def filter_images_by_boundary(om, folder, boundary_file, force_exiftags=False):
    #Load all of the images into a dictionary where the key is the image name and the value is the image path
    #Load the boundary into a shapely object
    #Filter the images by the boundary


    boundary = gpd.read_file(boundary_file)
    boundary.to_crs(epsg=4326, inplace=True)
    boundary = boundary.unary_union

    filtered_images = []
    #output_folder = folder + 'OUTPUT/'
    output_folder = folder
    omimage_folder = os.path.join(folder, f'OUTPUT_{om}')

    if os.path.exists(omimage_folder):
        # Move images back to OUTPUT folder
        for f in os.listdir(omimage_folder):
            if f.lower().endswith(('.tif', '.jpg', '.JPG', '.jpeg')):
                old_path = os.path.join(omimage_folder, f)
                new_path = os.path.join(output_folder, f)
                os.rename(old_path, new_path)
                print(f'Moved {old_path}  {new_path}')

            elif f.lower().endswith(('.csv')):
                #Delete the csv file
                os.remove(os.path.join(omimage_folder, f))
                print(f'Deleted {f}')
            else:
                print(f'Skipping {f}')

    # if it exists load csv file
    csv_files = glob.glob(os.path.join(output_folder, "*.csv"))

    


    # check if there is a csv file but it is empty
    if csv_files:
        try:
            df_csv = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)
            if df_csv.empty:
                print(f'Empty CSV file found: {csv_files[0]}')
                csv_files = []
        except pd.errors.EmptyDataError:
            print(f'CSV file exists but has no data: {csv_files[0]}')
            csv_files = []
        except Exception as e:
            print(f'Error reading CSV file {csv_files[0]}: {str(e)}')
            csv_files = []

    if force_exiftags or not csv_files:
        for i, f in enumerate(sorted(os.listdir(output_folder))):
            if f.lower().endswith(('.tif', '.jpg', '.JPG', '.jpeg')):
                image_path = os.path.join(output_folder, f)
                #print(f'Processing {f}')
                # get the latitude and longitude values from the image with exiftool
                metadata = get_image_metadata(image_path)
                lat, lon = metadata['latitude'], metadata['longitude']
                #print(f'lat = {lat}, lon = {lon}')
                if lat and lon:
                    point = Point(lon, lat)
                    if boundary.contains(point):
                        filtered_images.append(image_path)
                        if i % 100 == 0:
                            print(f'Image {f} is within the boundary')
                    else:
                        if i % 100 == 0:
                            print(f'Image {f} is outside the boundary')

    elif csv_files:
        # Read all CSV files into a single DataFrame
        df_csv = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)
        for i, f in enumerate(sorted(os.listdir(output_folder))):
            if f in df_csv['# image name'].values:
                image_path = os.path.join(output_folder, f)
                # Find the row in the csv file that matches the image name
                row = df_csv.loc[df_csv['# image name'] == f]
                if not row.empty:
                    lat = row['latitude [decimal degrees]'].values[0]
                    lon = row['longitude [decimal degrees]'].values[0]
                    if lat and lon:
                        point = Point(lon, lat)
                        if boundary.contains(point):
                            filtered_images.append(image_path)
                            if i % 100 == 0:
                                print(f'Image {f} is within the boundary')
                        else:
                            if i % 100 == 0:
                                print(f'Image {f} is outside the boundary')
                else:
                    print(f'No metadata found for {f}')

    print(f'returned filtered images: {len(filtered_images)}')
    return filtered_images

def move_images_to_folder(images, output_folder, om):
    # Move images within the specified boundary to a new folder
    omimage_folder = Path(output_folder).parents[0] / f'OUTPUT_{om}'

    print(f'moving images from {output_folder} to {omimage_folder}')

    # Create omimage folder if it doesn't exist
    try:
        os.makedirs(omimage_folder, exist_ok=True)
        print(f'Created {omimage_folder}')
    except Exception as e:
        print(f'Error creating {omimage_folder}: {e}')
        return

    for i, f in enumerate(os.listdir(output_folder)):
        if f.lower().endswith(('.tif', '.jpg', '.JPG', '.jpeg')):
            full_image_path = os.path.join(output_folder, f)
            if full_image_path in images:
                old_path = os.path.join(output_folder, f)
                new_path = os.path.join(omimage_folder, f)
                os.rename(old_path, new_path)
                if i % 100 == 0:
                    print(f'Moved {old_path}  {new_path}')






def consolidate_images(folder, location, sensor_type, sensor_name, date):
    #Move all images to a single folder
    #Find all folders for the sensor_type and date
    print(f'-------Multiple folders found')
    print(f'field = {location}')
    print(f'sensor_name = {sensor_name}')
    print(f'date = {date}')

    # Go up to the parent folder
    parent_folder = Path(folder).parents[2]
    print(f'parent_path: {parent_folder}')

    # Find all folders for the sensor_type case insensitive
    sensor_folders = sorted([f for f in os.listdir(parent_folder) if sensor_name in f.lower() and location in f.lower()])

    print(f'sensor_folders = {sensor_folders}')

    image_folders = []
    for sensor_folder in sensor_folders:
        image_folder = os.path.join(parent_folder, sensor_folder,'01_Images', sensor_folder, 'OUTPUT')
        image_folders.append(image_folder)
    print(f'image_folders = {image_folders}')

    #Check if the image folders already contain consolidate images with a pattern of '{number}_*.
    if any(file.lower().endswith(('.tif', '.jpg', '.jpeg')) for file in os.listdir(image_folder[0]) if re.match(r'^\d+_', file)):
        print(f'{image_folder[0]} already contains consolidated images. Skipping consolidation.')
        return

    # Move all images to first folder and prepend with folder number
    for i, image_folder in enumerate(image_folders):
        if i > 0:
            print(f'Processing {image_folder}')
            for file in os.listdir(image_folder):
                if file.lower().endswith(('.tif', '.jpg', '.jpeg')):
                    new_filepath = os.path.join(image_folders[0], f"{i+1}_{file}")
                    if not os.path.exists(new_filepath):
                        os.rename(os.path.join(image_folder, file), new_filepath)
                    else:
                        print(f'{new_filepath} already exists. Skipping {file} and ending consolidate_images function')
                        return
                    #print(f'Moving {file} to {image_folders[0]}')
                if file.lower().endswith('.csv'):
                    # Prepend image names with folder number
                    geotag_csv = pd.read_csv(os.path.join(image_folder, file))
                    geotag_csv['# image name'] = geotag_csv['# image name'].apply(lambda x: f"{i+1}_{x}")
                    geotag_csv.to_csv(os.path.join(image_folders[0], file), index=False)
                    print(f'moved csv file: {os.path.join(image_folder, file)}')
                    # Prepend image names with folder number

            # Rename the sensor_folder to keep from future runs trying to move images again, include sensor_type but not sensor_name so it is not found again.
            
            new_folder_name = os.path.join(Path(image_folder).parents[1],f'{i+1}_{date}_{location}_{sensor_type}_consolidated')
            os.rename(Path(image_folder).parents[0], new_folder_name)
            
            new_folder_name = os.path.join(Path(image_folder).parents[3],f'{i+1}_{date}_{location}_{sensor_type}_consolidated')
            os.rename(Path(image_folder).parents[2], new_folder_name)



            print(f'Renamed {image_folder} to {new_folder_name}')


def check_output_in_folders(folders):
    """
    Create a list indicating if 'OUTPUT' exists in each folder.

    :param folders: List of folder paths to check.
    :return: List of booleans indicating presence of 'OUTPUT' in each folder.
    """
    output_exists = []
    for folder in folders:
        output_path = os.path.join(folder, 'OUTPUT')
        # Check if 'OUTPUT' folder exists
        if os.path.isdir(output_path):
            # Check if there's any .pdf file in the 'OUTPUT' folder
            pdf_exists = any(file.lower().endswith('.pdf') for file in os.listdir(output_path) if os.path.isfile(os.path.join(output_path, file)))
            output_exists.append(pdf_exists)
        else:
            output_exists.append(False)
    return output_exists

def separate_folders_by_output(folders, output_flags):
    """
    Separate folders into two lists based on whether 'OUTPUT' exists.

    :param folders: List of folder paths.
    :param output_flags: List of booleans indicating presence of 'OUTPUT' in each folder.
    :return: Two lists - folders with 'OUTPUT' and folders without 'OUTPUT'.
    """
    folders_output = [folder for folder, exists in zip(folders, output_flags) if exists]
    folders_nooutput = [folder for folder, exists in zip(folders, output_flags) if not exists]
    return folders_output, folders_nooutput

def count_files_in_output(folders_output):
    """
    Count the number of files in each 'OUTPUT' folder.

    :param folders_output: List of folder paths that contain 'OUTPUT'.
    :return: A list of tuples with the folder path and the number of files in its 'OUTPUT' folder.
    """
    output_file_counts = []
    for folder in folders_output:
        output_path = os.path.join(folder, 'OUTPUT')
        if os.path.isdir(output_path):
            # Count only .tif or .jpg files, case-insensitive
            num_files = sum([
                1 for item in os.listdir(output_path)
                if os.path.isfile(os.path.join(output_path, item)) and
                item.lower().endswith(('.tif', '.jpg'))
            ])
            output_file_counts.append((output_path, num_files))
    return output_file_counts

def sort_folders_by_date(folders):
    """
    Sort folders by date based on the first 8 characters (assuming format yyyymmdd).

    :param folders: List of folder paths to sort.
    :return: Sorted list of folder paths.
    """
    # Define a function to extract the date part from the folder path
    def extract_date(folder):
        path_parts = folder.split('/')
        # Ensure path_parts[9] or equivalent exists before slicing to avoid errors
        if len(path_parts) > 9:
            return path_parts[6][:8]  # Adjust the index if needed based on folder structure
        return ''  # Return an empty string if the date part doesn't exist

    # Sort the folders based on the extracted date
    return sorted(folders, key=extract_date)

def write_folders_to_csv(file_path, folder_list):
    """
    Write a list of folder paths to a CSV file.

    :param file_path: The path to the output CSV file.
    :param folder_list: The list of folder paths to write.
    """

    folder_shortpath = []
    for folder, count in folder_list:
        path_parts = folder.split('/')
        #print(path_parts[9])
        short_path = path_parts[6] + '/' + path_parts[7]
        folder_shortpath.append((short_path, count))
    try:
        with open(file_path, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['Folder Path'])  # Write header
            for short_path, count in folder_shortpath:
                writer.writerow([short_path, count])
        print(f"CSV file written successfully at {file_path}")
    except Exception as e:
        print(f"An error occurred while writing to {file_path}: {e}")




if __name__ == "__main__":

# Example usage
    #path_to_list = '/Users/lukewaltz/Library/CloudStorage/OneDrive-TheOhioStateUniversity/ResearchProjects/20240501-uas/flights/'
    path_to_list = '/fs/ess/PAS2699/uas_data/flights/'
    print(f'path listing: {path_to_list}')
    folders = list_folders_two_levels_deep(path_to_list)

    # Check if 'OUTPUT' exists inside each path in the folders list
    output_flags = check_output_in_folders(folders)

    # Separate folders into two lists based on the presence of 'OUTPUT'
    folders_output, folders_nooutput = separate_folders_by_output(folders, output_flags)

    # Sort folders_nooutput by date
    folders_nooutput_sorted = sort_folders_by_date(folders_nooutput)
    folders_output_sorted = sort_folders_by_date(folders_output)

    # Count the number of files in each 'OUTPUT' folder
    output_file_counts = count_files_in_output(folders_output_sorted)

    # Write the sorted lists to separate CSV files
    write_folders_to_csv(path_to_list + 'notgeotagged.csv', [(folder, 0) for folder in folders_nooutput_sorted])  # Use 0 for file count
    write_folders_to_csv(path_to_list + 'geotagged.csv', output_file_counts)