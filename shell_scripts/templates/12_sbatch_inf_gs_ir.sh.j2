#!/bin/bash
#SBATCH -J "gs_ir_{{job_title}}"
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --output={{ log_dir }}/%j-{{job_title}}-inf_gs.txt
#SBATCH --error={{ log_dir }}/%j-{{job_title}}-inf_gs.err
#SBATCH --time=00:20:00
#SBATCH --mem=300G
#SBATCH --gpus-per-node=1
#SBATCH -A PAS2699

module load cuda/12.3.0

source /fs/ess/PAS2699/envs/miniconda3/etc/profile.d/conda.sh
conda activate harvest




echo "Running growth stage inference for: {{ job_title }}"

num_cores=$(nproc)
echo Number of cores: $num_cores

job_name="{{ job_title }}"

start_time=$(date +%s)

python -u "{{ python_script }}" \
    --input_dir "{{ gs_input_dir }}" \
    --output_dir "{{ output_path_gs }}" \
    --model_path "{{ model_path }}" \
    --field "{{ om }}" \
    --plotimage_source "{{ plotimage_source }}" \
    --date "{{ date }}"



end_time=$(date +%s)
execution_time=$((end_time - start_time))

processing_step="growth_stage_inference"

log_dir={{logdir_perf}}
log_file="$log_dir/execution_times_growth_stage_inference.csv"

# Get the size of the output folder
output_size=$(du -sh "{{ output_path_gs }}" | awk '{print $1}')

# Check if the log file exists
if [ ! -f "$log_file" ]; then
  # File doesn't exist, so write the header
  echo "job_id,job_name,processing_step,num_cores,memory,start_time,end_time,execution_time,output_size" > "$log_file"
fi

# Append the data
echo "$SLURM_JOB_ID,$job_name,$processing_step,$num_cores,$SLURM_MEM_PER_NODE,$start_time,$end_time,$execution_time,$output_size" >> "$log_file"

echo "All jobs are done, total time of execution: $execution_time seconds"